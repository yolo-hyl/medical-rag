# 完整应用配置示例
milvus:
  uri: "http://localhost:19530"
  token: null
  collection_name: "medical_knowledge2"
  drop_old: false
  auto_id: true

embedding:
  dense:
    provider: "ollama"
    model: "bge-m3:latest" 
    base_url: "http://localhost:11434"
    api_key: null
    proxy: null
    dimension: 1024
  sparse:
    manager: "self"  # self 或 milvus
    vocab_path: "vocab.pkl.gz"
    domain_model: "medicine"
    k1: 1.5
    b: 0.75
    workers: 8
    chunksize: 64

llm:
  provider: "ollama"
  model: "qwen3:32b"
  base_url: "http://localhost:11434"
  api_key: null
  proxy: null
  temperature: 0.1
  max_tokens: null

data:
  path: "/home/weihua/medical-rag/raw_data/raw/train/sample/qa_50000.jsonl"
  format: "jsonl"
  question_field: "question"
  answer_field: "answer"
  id_field: null
  source_field: null
  default_source: "unknown"
  batch_size: 100

search:
  top_k: 10
  score_threshold: null
  rrf_k: 100
  output_fields: ["question", "answer", "source"]
  filters: null

# 新增配置部分
ingestion:
  batch_size: 100
  max_workers: 4
  max_retries: 3
  build_vocab_if_needed: true
  clean_text: true
  remove_duplicates: true

annotation:
  max_concurrent: 3
  batch_size: 10
  model_backend: "ollama"
  model_base_url: "http://localhost:11434" 
  model_name: "qwen3:32b"
  departments_enabled: true
  categories_enabled: true
  
rag:
  mode: "basic"
  retrieval:
    top_k: 10
    dense_weight: 0.5
    sparse_weight: 0.5
  generation:
    temperature: 0.1
    include_sources: true
    max_sources: 3