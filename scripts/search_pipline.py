"""
æŸ¥è¯¢ Pipeline æµ‹è¯•è„šæœ¬
æ”¯æŒä½¿ç”¨ç‹¬ç«‹çš„æœç´¢é…ç½®æ–‡ä»¶
"""

import sys
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
from MedicalRag.pipeline.query.query_pipeline import QueryPipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_search_results(queries: List[str], results: List[List[Dict[str, Any]]]):
    """
    æ ¼å¼åŒ–æ‰“å°æœç´¢ç»“æœ
    
    Args:
        queries: æŸ¥è¯¢åˆ—è¡¨
        results: æœç´¢ç»“æœ
    """
    for i, (query, hits) in enumerate(zip(queries, results)):
        print(f"\n{'='*50}")
        print(f"æŸ¥è¯¢ [{i+1}]: {query}")
        print(f"{'='*50}")
        
        if not hits:
            print("âŒ æ— ç»“æœ")
            continue
            
        for j, hit in enumerate(hits):
            print(f"\nç»“æœ [{j+1}]:")
            print(f"  ID: {hit.get('id', 'N/A')}")
            print(f"  è·ç¦»: {hit.get('distance', 'N/A'):.4f}")
            print(f"  è¯„åˆ†: {hit.get('score', 'N/A'):.4f}")
            
            # æ‰“å°é…ç½®çš„è¾“å‡ºå­—æ®µ
            question = hit.get('question', '')
            answer = hit.get('answer', '')
            if question:
                print(f"  é—®é¢˜: {question[:100]}{'...' if len(question) > 100 else ''}")
            if answer:
                print(f"  ç­”æ¡ˆ: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            
            # æ‰“å°å…¶ä»–å­—æ®µ
            for key, value in hit.items():
                if key not in ['id', 'distance', 'score', 'question', 'answer']:
                    print(f"  {key}: {value}")


def test_pipeline_setup(config_path: str, use_search_config: bool = False) -> QueryPipeline:
    """
    æµ‹è¯• Pipeline è®¾ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        use_search_config: æ˜¯å¦ä½¿ç”¨æœç´¢ä¸“ç”¨é…ç½®
        
    Returns:
        QueryPipeline: è®¾ç½®å¥½çš„æŸ¥è¯¢ Pipeline
    """
    config_type = "æœç´¢ä¸“ç”¨" if use_search_config else "å®Œæ•´"
    logger.info(f"=== å¼€å§‹æµ‹è¯•æŸ¥è¯¢ Pipeline è®¾ç½®ï¼ˆ{config_type}é…ç½®ï¼‰ ===")
    
    try:
        # æ ¹æ®é…ç½®ç±»å‹åˆ›å»º Pipeline
        if use_search_config:
            pipeline = QueryPipeline.create_from_search_config(config_path)
        else:
            pipeline = QueryPipeline(config_path=config_path)
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        logger.info(f"é›†åˆåç§°: {pipeline.collection_name}")
        logger.info(f"Milvus URI: {pipeline.cfg.milvus.client.uri}")
        logger.info(f"åµŒå…¥æ¨¡å‹: {pipeline.cfg.embedding.dense.provider}/{pipeline.cfg.embedding.dense.model}")
        logger.info(f"ç¨€ç–å‘é‡: {pipeline.cfg.embedding.sparse_bm25.vocab_path}")
        logger.info(f"é…ç½®ç±»å‹: {pipeline._config_type}")
        
        # æ‰§è¡Œè®¾ç½®
        success = pipeline.setup()
        
        if success:
            logger.info("âœ… æŸ¥è¯¢ Pipeline è®¾ç½®æˆåŠŸ")
            
            # æ˜¾ç¤ºæœç´¢é…ç½®
            search_config = pipeline.get_search_config()
            logger.info("æœç´¢é…ç½®:")
            logger.info(f"  é»˜è®¤é™åˆ¶: {search_config['default_limit']}")
            logger.info(f"  è¾“å‡ºå­—æ®µ: {search_config['output_fields']}")
            logger.info(f"  RRFå¯ç”¨: {search_config['rrf_enabled']}")
            
            enabled_channels = [ch for ch in search_config['channels'] if ch['enabled']]
            logger.info(f"  å¯ç”¨é€šé“æ•°: {len(enabled_channels)}")
            for ch in enabled_channels:
                logger.info(f"    - {ch['name']} ({ch['kind']}, weight: {ch['weight']})")
            
            return pipeline
        else:
            logger.error("âŒ æŸ¥è¯¢ Pipeline è®¾ç½®å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
        return None


def test_single_query(pipeline: QueryPipeline, query: str, expr_vars: Dict[str, Any] = None):
    """
    æµ‹è¯•å•ä¸ªæŸ¥è¯¢
    
    Args:
        pipeline: æŸ¥è¯¢ Pipeline
        query: æŸ¥è¯¢æ–‡æœ¬
        expr_vars: è¡¨è¾¾å¼å˜é‡
    """
    logger.info(f"=== æµ‹è¯•å•ä¸ªæŸ¥è¯¢: {query} ===")
    
    try:
        results = pipeline.search_single(query, expr_vars=expr_vars)
        
        logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        print_search_results([query], [results])
        
        return True
    except Exception as e:
        logger.error(f"âŒ å•ä¸ªæŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_batch_query(pipeline: QueryPipeline, queries: List[str], expr_vars: Dict[str, Any] = None):
    """
    æµ‹è¯•æ‰¹é‡æŸ¥è¯¢
    
    Args:
        pipeline: æŸ¥è¯¢ Pipeline
        queries: æŸ¥è¯¢åˆ—è¡¨
        expr_vars: è¡¨è¾¾å¼å˜é‡
    """
    logger.info(f"=== æµ‹è¯•æ‰¹é‡æŸ¥è¯¢ï¼Œå…± {len(queries)} ä¸ªæŸ¥è¯¢ ===")
    
    try:
        results = pipeline.search(queries, expr_vars=expr_vars)
        
        total_results = sum(len(r) for r in results)
        logger.info(f"æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œæ€»å…±è¿”å› {total_results} ä¸ªç»“æœ")
        
        print_search_results(queries, results)
        
        return True
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_filtered_query(pipeline: QueryPipeline, query: str):
    """
    æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶çš„æŸ¥è¯¢
    
    Args:
        pipeline: æŸ¥è¯¢ Pipeline
        query: æŸ¥è¯¢æ–‡æœ¬
    """
    logger.info(f"=== æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶çš„æŸ¥è¯¢: {query} ===")
    
    try:
        # æµ‹è¯•ä¸åŒçš„è¿‡æ»¤æ¡ä»¶
        test_cases = [
            {"src": "huatuo_qa"},
            {"dept": "0"},
            {"src": "huatuo_qa", "dept": "1"}
        ]
        
        for i, expr_vars in enumerate(test_cases):
            logger.info(f"æµ‹è¯•è¿‡æ»¤æ¡ä»¶ [{i+1}]: {expr_vars}")
            results = pipeline.search_single(query, expr_vars=expr_vars)
            logger.info(f"  ç»“æœæ•°é‡: {len(results)}")
            
            if results:
                logger.info(f"  ç¬¬ä¸€ä¸ªç»“æœID: {results[0].get('id', 'N/A')}")
        
        return True
    except Exception as e:
        logger.error(f"âŒ è¿‡æ»¤æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_channel_update(pipeline: QueryPipeline):
    """
    æµ‹è¯•åŠ¨æ€æ›´æ–°æœç´¢é€šé“
    
    Args:
        pipeline: æŸ¥è¯¢ Pipeline
    """
    logger.info("=== æµ‹è¯•åŠ¨æ€æ›´æ–°æœç´¢é€šé“ ===")
    
    try:
        # è·å–å½“å‰é…ç½®
        original_config = pipeline.get_search_config()
        logger.info("åŸå§‹é€šé“é…ç½®:")
        for ch in original_config['channels']:
            if ch['enabled']:
                logger.info(f"  {ch['name']}: enabled={ch['enabled']}, weight={ch['weight']}")
        
        # æµ‹è¯•æ›´æ–°é€šé“æƒé‡
        channel_updates = [
            {"name": "sparse_doc", "weight": 0.6},
            {"name": "sparse_q", "weight": 0.4}
        ]
        
        success = pipeline.update_search_channels(channel_updates)
        if success:
            logger.info("âœ… é€šé“é…ç½®æ›´æ–°æˆåŠŸ")
            
            # æ˜¾ç¤ºæ›´æ–°åçš„é…ç½®
            updated_config = pipeline.get_search_config()
            logger.info("æ›´æ–°åé€šé“é…ç½®:")
            for ch in updated_config['channels']:
                if ch['enabled']:
                    logger.info(f"  {ch['name']}: enabled={ch['enabled']}, weight={ch['weight']}")
        else:
            logger.error("âŒ é€šé“é…ç½®æ›´æ–°å¤±è´¥")
            
        return success
    except Exception as e:
        logger.error(f"âŒ é€šé“æ›´æ–°æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_config_dict_creation():
    """
    æµ‹è¯•ä»é…ç½®å­—å…¸åˆ›å»ºPipeline
    """
    logger.info("=== æµ‹è¯•ä»é…ç½®å­—å…¸åˆ›å»ºPipeline ===")
    
    try:
        # ç¤ºä¾‹é…ç½®å­—å…¸
        config_dict = {
            "milvus": {
                "client": {
                    "uri": "http://localhost:19530",
                    "token": "root:Milvus",
                    "db_name": "",
                    "timeout_ms": 30000,
                    "tls": False,
                    "tls_verify": False
                },
                "collection": {
                    "name": "qa_knowledge",
                    "description": "RAG QA knowledge base",
                    "load_on_start": True
                }
            },
            "search": {
                "default_limit": 5,
                "output_fields": ["question", "answer"],
                "pagination": {"page_size": 10, "max_pages": 100},
                "expr_template": "",
                "rrf": {"enabled": False, "k": 100},
                "channels": [
                    {
                        "name": "sparse_doc",
                        "field": "sparse_vec_qa",
                        "enabled": True,
                        "kind": "sparse_document",
                        "metric_type": "IP",
                        "limit": 5,
                        "params": {"drop_ratio_search": 0.0},
                        "expr_template": "",
                        "weight": 1.0
                    }
                ]
            },
            "embedding": {
                "dense": {
                    "provider": "ollama",
                    "model": "nomic-embed-text",
                    "base_url": "http://172.16.40.51:11434",
                    "dim": 768,
                    "normalize": False,
                    "prefixes": {"query": "search_query:", "document": "search_document:"}
                },
                "sparse_bm25": {
                    "vocab_path": "vocab.pkl.gz",
                    "domain_model": "medicine",
                    "prune_empty_sparse": True,
                    "empty_sparse_fallback": {"0": 0.0},
                    "stopwords": ["ä»€ä¹ˆ", "ï¼Ÿ", "æ˜¯", "å¦‚ä½•"],
                    "k1": 1.5,
                    "b": 0.75
                }
            }
        }
        
        # ä»å­—å…¸åˆ›å»ºPipeline
        pipeline = QueryPipeline.create_from_config_dict(config_dict)
        logger.info("âœ… ä»é…ç½®å­—å…¸åˆ›å»ºPipelineæˆåŠŸ")
        logger.info(f"é…ç½®ç±»å‹: {pipeline._config_type}")
        
        return True
    except Exception as e:
        logger.error(f"âŒ ä»é…ç½®å­—å…¸åˆ›å»ºPipelineå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æŸ¥è¯¢ Pipeline æµ‹è¯•è„šæœ¬")
    parser.add_argument(
        "-c", "--config", 
        default="src/MedicalRag/config/milvus.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "-s", "--search-config",
        default="src/MedicalRag/config/search/search_answer.yaml",
        help="æœç´¢é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--use-search-config",
        action="store_true",
        help="ä½¿ç”¨æœç´¢ä¸“ç”¨é…ç½®æ–‡ä»¶"
    )
    parser.add_argument(
        "-q", "--query",
        default="æ¢…æ¯’",
        help="æµ‹è¯•æŸ¥è¯¢æ–‡æœ¬"
    )
    parser.add_argument(
        "--batch-queries",
        nargs="+",
        default=["æ¢…æ¯’", "å·¨è‚ ç—‡æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ", "æœ€å¹¿æ³›çš„æ€§ç—…æ˜¯ä»€ä¹ˆï¼Ÿ"],
        help="æ‰¹é‡æŸ¥è¯¢åˆ—è¡¨"
    )
    parser.add_argument(
        "--setup-only", 
        action="store_true",
        help="ä»…æµ‹è¯•Pipelineè®¾ç½®ï¼Œä¸æ‰§è¡ŒæŸ¥è¯¢"
    )
    parser.add_argument(
        "--test-filter", 
        action="store_true",
        help="æµ‹è¯•è¿‡æ»¤æŸ¥è¯¢"
    )
    parser.add_argument(
        "--test-update", 
        action="store_true",
        help="æµ‹è¯•åŠ¨æ€æ›´æ–°é€šé“é…ç½®"
    )
    parser.add_argument(
        "--test-config-dict",
        action="store_true",
        help="æµ‹è¯•ä»é…ç½®å­—å…¸åˆ›å»ºPipeline"
    )
    
    args = parser.parse_args()
    
    # é€‰æ‹©é…ç½®æ–‡ä»¶
    config_path = args.search_config if args.use_search_config else args.config
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_file = Path(config_path)
    if not config_file.exists():
        logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        logger.info("è¯·ç¡®ä¿é…ç½®æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        sys.exit(1)
    
    config_type = "æœç´¢ä¸“ç”¨" if args.use_search_config else "å®Œæ•´"
    logger.info(f"ä½¿ç”¨{config_type}é…ç½®æ–‡ä»¶: {config_path}")
    
    try:
        success_count = 0
        total_tests = 0
        
        # æµ‹è¯•ä»é…ç½®å­—å…¸åˆ›å»ºPipeline
        if args.test_config_dict:
            total_tests += 1
            if test_config_dict_creation():
                success_count += 1
        
        # è®¾ç½® Pipeline
        pipeline = test_pipeline_setup(str(config_path), args.use_search_config)
        if not pipeline:
            logger.error("Pipeline è®¾ç½®å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            sys.exit(1)
        
        if args.setup_only:
            logger.info("ğŸ‰ Pipeline è®¾ç½®æµ‹è¯•å®Œæˆ")
            sys.exit(0)
        
        # æµ‹è¯•å•ä¸ªæŸ¥è¯¢
        total_tests += 1
        if test_single_query(pipeline, args.query, expr_vars={'src': 'huatuo'}):
            success_count += 1
        
        # æµ‹è¯•æ‰¹é‡æŸ¥è¯¢
        total_tests += 1
        if test_batch_query(pipeline, args.batch_queries, expr_vars={'src': 'huatuo'}):
            success_count += 1
        
        # æµ‹è¯•è¿‡æ»¤æŸ¥è¯¢
        if args.test_filter:
            total_tests += 1
            if test_filtered_query(pipeline, args.query):
                success_count += 1
        
        # æµ‹è¯•é€šé“æ›´æ–°
        if args.test_update:
            total_tests += 1
            if test_channel_update(pipeline):
                success_count += 1
        
        # è¾“å‡ºæµ‹è¯•ç»“æœ
        logger.info(f"=== æµ‹è¯•å®Œæˆ: {success_count}/{total_tests} é€šè¿‡ ===")
        
        if success_count == total_tests:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡")
            sys.exit(0)
        else:
            logger.error("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()